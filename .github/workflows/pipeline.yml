name: aqi-pipeline

on:
  schedule:
    - cron: "0 */6 * * *"   # every 6 hours (reduced frequency to avoid rate limits)
  workflow_dispatch:        # manual trigger

permissions:
  contents: write

env:
  PYTHONPATH: ${{ github.workspace }}

jobs:
  feature_job:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create data directory
        run: mkdir -p data
      
      - name: Run feature ingestion
        run: python scripts/run_feature_job.py --city "Delhi"
        continue-on-error: false
      
      - name: Verify features file exists
        run: |
          if [ -f "data/features.csv" ]; then
            echo "Features file created successfully"
            wc -l data/features.csv
          else
            echo "ERROR: Features file not created!"
            exit 1
          fi
      
      - name: Upload features
        uses: actions/upload-artifact@v4
        with:
          name: features
          path: data/features.csv
          if-no-files-found: error
          retention-days: 7

  train_job:
    needs: feature_job
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Create directories
        run: |
          mkdir -p data
          mkdir -p models
          mkdir -p reports
      
      - name: Download features
        uses: actions/download-artifact@v4
        with:
          name: features
          path: data
      
      # Try to restore previous metrics history from a previous workflow run
      - name: Download previous metrics history
        uses: dawidd6/action-download-artifact@v6
        with:
          name: metrics-history
          path: reports
          workflow: pipeline.yml
          workflow_conclusion: success
          if_no_artifact_found: warn
          search_artifacts: true
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Train model
        run: python scripts/run_training_job.py
      
      - name: Display metrics history
        run: |
          echo "=== Metrics History ==="
          if [ -f "reports/metrics_history.csv" ]; then
            cat reports/metrics_history.csv
          else
            echo "No metrics history yet (first run)"
          fi
      
      - name: Verify model files exist
        run: |
          if [ -f "models/latest_model.pkl" ]; then
            echo "Model file created successfully"
            ls -la models/
          else
            echo "ERROR: Model file not created!"
            exit 1
          fi
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model
          path: |
            models/latest_model.pkl
            models/latest_shap.pkl
          if-no-files-found: error
          retention-days: 30
      
      - name: Upload metrics history
        uses: actions/upload-artifact@v4
        with:
          name: metrics-history
          path: reports/metrics_history.csv
          if-no-files-found: warn
          retention-days: 90
      
      # Commit metrics back to repository so Streamlit Cloud can access them
      - name: Commit metrics to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add reports/metrics_history.csv || true
          git diff --staged --quiet || git commit -m "chore: update metrics history [skip ci]"
          git push || echo "Nothing to push"
