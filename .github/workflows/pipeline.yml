name: aqi-pipeline

on:
  schedule:
    - cron: "0 */6 * * *"   # every 6 hours
  workflow_dispatch:        # manual trigger
  push:
    branches: [main]
  pull_request:
    branches: [main]

permissions:
  contents: write

env:
  PYTHONPATH: ${{ github.workspace }}

jobs:
  # ===========================================================================
  # Stage 1: Code Quality (Lint & Format Check)
  # ===========================================================================
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install flake8 black
      
      - name: Check code formatting with Black
        run: black --check --diff src/ api/ flows/ tests/ || echo "Formatting issues found"
        continue-on-error: true
      
      - name: Lint with flake8
        run: |
          flake8 src/ api/ flows/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src/ api/ flows/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

  # ===========================================================================
  # Stage 2: Unit Tests
  # ===========================================================================
  test:
    runs-on: ubuntu-latest
    needs: lint
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create directories
        run: |
          mkdir -p data models reports
      
      - name: Run unit tests
        run: |
          pytest tests/ -v --tb=short --ignore=tests/test_data_integrity.py || echo "Some tests skipped (no data)"
        continue-on-error: true
      
      - name: Run tests with coverage
        run: |
          pytest tests/test_api.py -v --cov=api --cov-report=xml || true
        continue-on-error: true

  # ===========================================================================
  # Stage 3: Data Ingestion (Feature Job)
  # ===========================================================================
  feature_job:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Create data directory
        run: mkdir -p data
      
      - name: Run feature ingestion for Pakistan cities
        run: |
          cities=("Karachi" "Lahore" "Islamabad" "Rawalpindi" "Faisalabad" "Multan" "Peshawar" "Quetta" "Sialkot" "Gujranwala" "Hyderabad" "Bahawalpur" "Sargodha" "Sukkur" "Larkana" "Sheikhupura" "Mirpur Khas" "Rahim Yar Khan" "Gujrat" "Jhang")
          for city in "${cities[@]}"; do
            echo "Fetching data for $city..."
            python scripts/run_feature_job.py --city "$city" || echo "Warning: Failed to fetch $city, continuing..."
            sleep 1  # Rate limiting
          done
        continue-on-error: false
      
      - name: Verify features file exists
        run: |
          if [ -f "data/features.csv" ]; then
            echo "Features file created successfully"
            wc -l data/features.csv
          else
            echo "ERROR: Features file not created!"
            exit 1
          fi
      
      - name: Upload features
        uses: actions/upload-artifact@v4
        with:
          name: features
          path: data/features.csv
          if-no-files-found: error
          retention-days: 7

  # ===========================================================================
  # Stage 4: Model Training
  # ===========================================================================
  train_job:
    needs: feature_job
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: 'pip'
      
      - name: Create directories
        run: |
          mkdir -p data
          mkdir -p models
          mkdir -p reports
      
      - name: Download features
        uses: actions/download-artifact@v4
        with:
          name: features
          path: data
      
      # Try to restore previous metrics history from a previous workflow run
      - name: Download previous metrics history
        uses: dawidd6/action-download-artifact@v6
        with:
          name: metrics-history
          path: reports
          workflow: pipeline.yml
          workflow_conclusion: success
          if_no_artifact_found: warn
          search_artifacts: true
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Train model
        run: python scripts/run_training_job.py
      
      - name: Run ML validation tests
        run: |
          pytest tests/test_model.py -v --tb=short || echo "Some model tests skipped"
        continue-on-error: true
      
      - name: Display metrics history
        run: |
          echo "=== Metrics History ==="
          if [ -f "reports/metrics_history.csv" ]; then
            cat reports/metrics_history.csv
          else
            echo "No metrics history yet (first run)"
          fi
      
      - name: Verify model files exist
        run: |
          if [ -f "models/latest_model.pkl" ]; then
            echo "Model file created successfully"
            ls -la models/
          else
            echo "ERROR: Model file not created!"
            exit 1
          fi
      
      - name: Upload model artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model
          path: |
            models/latest_model.pkl
            models/latest_shap.pkl
          if-no-files-found: error
          retention-days: 30
      
      - name: Upload metrics history
        uses: actions/upload-artifact@v4
        with:
          name: metrics-history
          path: reports/metrics_history.csv
          if-no-files-found: warn
          retention-days: 90
      
      # Commit metrics back to repository so Streamlit Cloud can access them
      - name: Commit metrics to repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add reports/metrics_history.csv || true
          git diff --staged --quiet || git commit -m "chore: update metrics history [skip ci]"
          git push || echo "Nothing to push"

  # ===========================================================================
  # Stage 5: Build Container (optional for registry push)
  # ===========================================================================
  build:
    runs-on: ubuntu-latest
    needs: [lint, test]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker image
        run: |
          docker build -t aqi-predictor-api:latest .
          docker build -t aqi-predictor-dashboard:latest -f Dockerfile.streamlit .
      
      - name: Test container starts
        run: |
          docker run -d --name test-api -p 8000:8000 aqi-predictor-api:latest
          sleep 10
          curl -f http://localhost:8000/ || echo "API health check (container may need model)"
          docker stop test-api
